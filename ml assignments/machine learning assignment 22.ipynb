{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050c9ef2",
   "metadata": {},
   "source": [
    "# 1.Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?\n",
    "sol.\n",
    "\n",
    "try combining them into a voting ensemble, which will often give you even better results. It works better if the models are very different\n",
    "\n",
    " # 2. What's the difference between hard voting classifiers and soft voting classifiers?\n",
    "sol.\n",
    "\n",
    "In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels. In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n",
    "\n",
    "   # 3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options.\n",
    "sol.\n",
    "\n",
    "# 4. What is the advantage of evaluating out of the bag?\n",
    "sol. the advantage of evaluating out of bag are that the model is never trained on the that data so it is used for evaluating the accuracy of the model. so,we do not need of cross validation and can use out of bag instance for that purpose.\n",
    "\n",
    "# 5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?\n",
    "\n",
    "Extra Trees is like a Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits. So in summary, ExtraTrees:\n",
    "\n",
    "builds multiple trees with bootstrap = False by default, which means it samples without replacement\n",
    "nodes are split based on random splits among a random subset of the features selected at every node\n",
    "In Extra Trees, randomness doesnâ€™t come from bootstrapping the data, but rather comes from the random splits of all observations. ExtraTrees is named for (Extremely Randomized Trees).\n",
    "The Extra-(Randomized)-Trees (ET) a bias-variance analysis\n",
    "\n",
    "Both methods are about the same, with the ET being a bit worse when there is a high number of noisy features (in high dimensional data-sets).\n",
    "\n",
    "That said, provided the (perhaps manual) feature selection is near optimal, the performance is about the same, however, ET's can be computationally faster.\n",
    "\n",
    "# 6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?\n",
    "sol. try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator, also try slightly increasing the learning rate.\n",
    "\n",
    "# 7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?\n",
    "sol. decreasing the learning rate, early stopping to find the right number of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b967a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
